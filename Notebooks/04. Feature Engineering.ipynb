{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74269821-9ea3-47bc-8a42-91159f5e79be",
   "metadata": {},
   "source": [
    "# Feature Engineering Notebook: Mental Health Text Classification\n",
    "This notebook presents a robust, production-ready feature engineering pipeline for mental health text classification. The goal is to transform raw user-generated statements into a comprehensive set of interpretable and machine-learning-ready features, enabling accurate categorization into five key mental health states: Anxiety, Depression, Normal, Stress, and Suicidal.\r\n",
    "\r\n",
    "The approach is designed for maximum portability and reliability, using only pure-Python and lightweight NLP libraries. This ensures seamless deployment in containerized and cloud environments, such as Google Cloud Platform (GCP), without dependency on GPU drivers or complex C/C++ libraries.\r\n",
    "\r\n",
    "**Key steps include:**\r\n",
    "- Rigorous text cleaning and normalization to ensure privacy and consistency.\r\n",
    "- Extraction of structural, lexical, sentiment, and domain-specific features that capture both the style and substance of mental health discourse.\r\n",
    "- Heuristic part-of-speech (POS) analysis and TF-IDF vectorization to provide both interpretable and high-dimensional representations.\r\n",
    "- Careful feature scaling and export, supporting a wide range of downstream machine learning models.\r\n",
    "\r\n",
    "This notebook serves as a critical foundation for subsequent model development, evaluation, and deployment in real-world mental health support systems.y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3565ae0-18a5-43cd-af9d-15136dab7b6b",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f65ea3da-f21f-4aea-8f16-476219f44b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete (pure-Python fallback)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Always disable NLTK usage for tokenization/POS to avoid punkt errors\n",
    "use_nltk = False\n",
    "\n",
    "# Fallback stopword set\n",
    "stopset = {\n",
    "    'the','and','is','in','to','it','of','for','on','with','as','this',\n",
    "    'a','an','that','at','by','from','or','be','are','was','were'\n",
    "}\n",
    "\n",
    "print(\"Environment setup complete (pure-Python fallback)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2904f-5470-45e0-912b-3601604f987a",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "431c938c-1beb-4143-9e90-e2e14f7a625b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned_dataset.csv...\n",
      "Dataset loaded: 45261 records\n",
      "Class distribution:\n",
      "status\n",
      "Normal                  15991\n",
      "Depression              12106\n",
      "Suicidal                 8812\n",
      "Anxiety                  3103\n",
      "Stress                   2343\n",
      "Bipolar                  2118\n",
      "Personality disorder      788\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading cleaned_dataset.csv...\")\n",
    "try:\n",
    "    df = pd.read_csv('cleaned_dataset.csv')\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load data: {e}\")\n",
    "\n",
    "df = df.rename(columns={'Unnamed: 0': 'Unique_ID'})\n",
    "df = df[['Unique_ID', 'statement', 'status']]\n",
    "print(f\"Dataset loaded: {len(df)} records\")\n",
    "print(\"Class distribution:\")\n",
    "print(df['status'].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf00fb74-ba4f-4d3d-b868-06f002c58c58",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning\n",
    "Remove HTML tags, mask URLs and emojis, normalize whitespace, lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "185836f0-8c18-4084-a13e-edc74e45c023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text...\n",
      "Completed in 15.89s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    t = str(text)\n",
    "    t = re.sub(r\"<.*?>\", \" \", t)\n",
    "    t = re.sub(r\"http\\S+|www\\S+\", \" urltoken \", t)\n",
    "    t = emoji.replace_emoji(t, replace=\" emoticon \")\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t.lower().strip()\n",
    "\n",
    "print(\"Cleaning text...\")\n",
    "start = time.time()\n",
    "df['clean_text'] = df['statement'].apply(clean_text)\n",
    "print(f\"Completed in {time.time() - start:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8370a4e1-fff1-485b-bbe7-6d07a0b81da4",
   "metadata": {},
   "source": [
    "*Decision Note: We perform cleaning here to handle noisy characters, privacy-sensitive content, and ensure all downstream features are reliable.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d64640-cbcd-4d11-8e0b-ff453fd59e83",
   "metadata": {},
   "source": [
    "## 4. Structural Features\n",
    "Extract metrics: text length, word count, URL count, emoji count, special chars,  punctuation bursts, average word length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bdca9e5d-de9d-43a9-998e-50f484a92dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting structural features...\n",
      "Completed in 16.31s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting structural features...\")\n",
    "start = time.time()\n",
    "df['text_length'] = df['clean_text'].str.len()\n",
    "df['word_count'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
    "df['num_urls'] = df['statement'].apply(lambda x: len(re.findall(r\"http\\S+|www\\S+\", str(x))))\n",
    "df['num_emojis'] = df['statement'].apply(lambda x: emoji.emoji_count(str(x)))\n",
    "df['num_special_chars'] = df['statement'].apply(lambda x: sum(ord(c) > 126 for c in str(x)))\n",
    "df['num_excess_punct'] = df['statement'].apply(lambda x: len(re.findall(r\"[.!?]{3,}\", str(x))))\n",
    "df['avg_word_length'] = df['clean_text'].apply(\n",
    "    lambda x: np.mean([len(w) for w in x.split()]) if x.split() else 0\n",
    ")\n",
    "print(f\"Completed in {time.time() - start:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f73d2-a832-404a-9b49-dfbd0142f31c",
   "metadata": {},
   "source": [
    "*Decision Note: These features capture information density, use of digital content, and structural quirks of user statements.*\n",
    "\n",
    "### Interpretation:\n",
    "*Statements with high emoji or special character counts may correspond to more expressive or emotional content.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f3f0b-41a8-4b68-b6b2-6bbeda4ee530",
   "metadata": {},
   "source": [
    "## 5. Lexical Features \n",
    "Compute stopword ratio and type-token ratio for vocabulary richness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "026e0ee4-4b2f-48cb-ab96-9511bcf1abef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting lexical features...\n",
      "Completed in 1.27s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting lexical features...\")\n",
    "start = time.time()\n",
    "df['stopword_ratio'] = df['clean_text'].apply(\n",
    "    lambda x: sum(1 for w in x.split() if w in stopset) / max(1, len(x.split()))\n",
    ")\n",
    "df['type_token_ratio'] = df['clean_text'].apply(\n",
    "    lambda x: len(set(x.split())) / max(1, len(x.split()))\n",
    ")\n",
    "print(f\"Completed in {time.time() - start:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec7b36-0442-428c-a211-d93ac346b563",
   "metadata": {},
   "source": [
    "*Decision Note: Vocabulary richness helps distinguish between vague and highly-detailed posts, often correlated to mental health states.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81538998-887f-431b-a8db-59d841e271ec",
   "metadata": {},
   "source": [
    "## 6. Sentiment and Domain-Specific Features\n",
    "Use TextBlob for polarity and subjectivity. Binary flags for suicidal, stress, and help-seeking keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a793cd4f-5019-422b-acf1-3428cd68de82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sentiment features...\n",
      "Completed in 32.79s\n",
      "\n",
      "Extracting keyword flags...\n",
      "Completed in 1.63s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting sentiment features...\")\n",
    "start = time.time()\n",
    "from textblob import TextBlob\n",
    "df[['polarity', 'subjectivity']] = df['clean_text'].apply(\n",
    "    lambda x: pd.Series(TextBlob(x).sentiment)\n",
    ")\n",
    "print(f\"Completed in {time.time() - start:.2f}s\\n\")\n",
    "\n",
    "print(\"Extracting keyword flags...\")\n",
    "start = time.time()\n",
    "suicidal_kw = {'kill','die','suicide','self-harm','end it','dead'}\n",
    "stress_kw = {'stress','overwhelmed','pressure','burnout','anxious'}\n",
    "help_kw = {'help','save me','talk','support','therapist'}\n",
    "\n",
    "def flag_keywords(text, keywords):\n",
    "    lw = text.lower()\n",
    "    words = set(lw.split())\n",
    "    if words & keywords:\n",
    "        return 1\n",
    "    return any(phrase in lw for phrase in keywords if ' ' in phrase)\n",
    "\n",
    "df['has_suicidal_keyword'] = df['clean_text'].apply(lambda x: flag_keywords(x, suicidal_kw))\n",
    "df['has_stress_keyword']   = df['clean_text'].apply(lambda x: flag_keywords(x, stress_kw))\n",
    "df['has_help_keyword']     = df['clean_text'].apply(lambda x: flag_keywords(x, help_kw))\n",
    "print(f\"Completed in {time.time() - start:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca712e44-ee83-4ad8-a897-f75bd6d355bf",
   "metadata": {},
   "source": [
    "*Decision Note: Specific keyword flags allow for easy triage and prioritization of high-risk statements in a production setting.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d92ecc-e73e-4713-a20e-b370af7b35ac",
   "metadata": {},
   "source": [
    "## 7. POS Ratios Transformer\n",
    "Compute noun, verb, adjective, adverb ratios via NLTK or suffix heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3c89fc85-4b4b-4a6c-b1a7-5dde2932340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting POS ratio features...\n",
      "POS ratio features extracted in 9.88s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting POS ratio features...\")\n",
    "start = time.time()\n",
    "\n",
    "def compute_pos_ratios(text):\n",
    "    words = text.split()\n",
    "    total = len(words) or 1\n",
    "    return pd.Series({\n",
    "        'noun_ratio': sum(w.endswith(('ion','ment'))   for w in words) / total,\n",
    "        'verb_ratio': sum(w.endswith(('ing','ed'))    for w in words) / total,\n",
    "        'adj_ratio' : sum(w.endswith(('y','al'))      for w in words) / total,\n",
    "        'adv_ratio' : sum(w.endswith('ly')            for w in words) / total\n",
    "    })\n",
    "\n",
    "pos_df = df['clean_text'].apply(compute_pos_ratios)\n",
    "df = pd.concat([df, pos_df], axis=1)\n",
    "\n",
    "print(f\"POS ratio features extracted in {time.time()-start:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510b266-b330-4d8a-91d0-387d28424242",
   "metadata": {},
   "source": [
    "*Decision Note: POS ratios highlight syntactic style, which has been shown to indicate mood and intent.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009001d6-5c48-48d4-abf6-e86279d31441",
   "metadata": {},
   "source": [
    "## 8. TF-IDF Vectorization\n",
    "Fit a TF-IDF vectorizer on unigrams and bigrams (5,000 features max) with sublinear scaling and document frequency thresholds to create sparse bag-of-words features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8a09fbfe-59ee-4320-a2ac-e5cd6c5033c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating TF-IDF features...\n",
      "TF-IDF matrix shape: (45261, 5000)\n",
      "TF-IDF generation completed in 9.51s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating TF-IDF features...\")\n",
    "start = time.time()\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "tfidf_matrix = tfidf.fit_transform(df['clean_text'])\n",
    "tfidf_vocab  = tfidf.get_feature_names_out()\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"TF-IDF generation completed in {time.time()-start:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a61e274-87eb-493f-aee2-cad1d0d4f691",
   "metadata": {},
   "source": [
    "*Decision Note: TF-IDF provides a lightweight, interpretable representation of important terms without requiring deep models or GPUs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38fbe91-f20d-43e5-8a2f-da8afb5d22af",
   "metadata": {},
   "source": [
    "## 10. Feature Scaling\n",
    "Apply Minâ€“Max scaling to all numeric features (structural, lexical, sentiment, POS ratios) to bound them between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "76fa7367-dc4e-46f5-a1d7-0be35a69945e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling numeric features...\n",
      "Feature scaling completed in 0.02s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Scaling numeric features...\")\n",
    "start = time.time()\n",
    "\n",
    "numeric_cols = [\n",
    "    'text_length','word_count','num_urls','num_emojis','num_special_chars',\n",
    "    'num_excess_punct','avg_word_length','stopword_ratio','type_token_ratio',\n",
    "    'polarity','subjectivity','noun_ratio','verb_ratio','adj_ratio','adv_ratio'\n",
    "]\n",
    "scaler = MinMaxScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "print(f\"Feature scaling completed in {time.time()-start:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba7b42-42e6-48b1-b8f4-6b335c67a81b",
   "metadata": {},
   "source": [
    "*Decision Note: Uniform feature scaling prevents variables with larger magnitudes from dominating model learning and ensures stable optimization.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5573c52d-433a-4d24-8dda-e377d596fb0c",
   "metadata": {},
   "source": [
    "## 11. Export Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0bc16dce-beb3-42e0-9e60-a01cd872c03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting feature sets...\n",
      "Export complete:\n",
      "  - features_interpretable.csv (includes raw text column for deep models)\n",
      "  - tfidf_features.npz\n",
      "  - tfidf_vocab.csv\n",
      "\n",
      "Feature engineering pipeline finished successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Exporting feature sets...\")\n",
    "\n",
    "# Add text column to inter_cols for modeling with DistilBERT\n",
    "inter_cols = ['Unique_ID', 'statement', 'status'] + numeric_cols + [\n",
    "    'has_suicidal_keyword', 'has_stress_keyword', 'has_help_keyword'\n",
    "]\n",
    "\n",
    "df[inter_cols].to_csv('features_interpretable.csv', index=False)\n",
    "\n",
    "save_npz('tfidf_features.npz', tfidf_matrix)\n",
    "pd.DataFrame({'term': tfidf_vocab}).to_csv('tfidf_vocab.csv', index=False)\n",
    "\n",
    "print(\"Export complete:\")\n",
    "print(\"  - features_interpretable.csv (includes raw text column for deep models)\")\n",
    "print(\"  - tfidf_features.npz\")\n",
    "print(\"  - tfidf_vocab.csv\\n\")\n",
    "\n",
    "print(\"Feature engineering pipeline finished successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a085a629-8057-4c55-a692-6608ef97813c",
   "metadata": {},
   "source": [
    "*Decision Note: The feature set is now ready for modeling. Export ensures easy reuse and versioning in later ML stages.*\n",
    "\n",
    "# Interpretation:\n",
    "The notebook is modular and reproducible for future datasets, with all steps explained and decisions justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9d9adcc6-4c11-4fd9-a112-def4bd5bab85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>num_urls</th>\n",
       "      <th>num_emojis</th>\n",
       "      <th>num_special_chars</th>\n",
       "      <th>num_excess_punct</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>stopword_ratio</th>\n",
       "      <th>type_token_ratio</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "      <th>adj_ratio</th>\n",
       "      <th>adv_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "      <td>45261.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>26237.805440</td>\n",
       "      <td>0.276083</td>\n",
       "      <td>0.247557</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.010863</td>\n",
       "      <td>0.198562</td>\n",
       "      <td>0.807242</td>\n",
       "      <td>0.504900</td>\n",
       "      <td>0.452480</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>0.056079</td>\n",
       "      <td>0.069837</td>\n",
       "      <td>0.021898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15279.138295</td>\n",
       "      <td>0.264196</td>\n",
       "      <td>0.236354</td>\n",
       "      <td>0.027201</td>\n",
       "      <td>0.018873</td>\n",
       "      <td>0.013427</td>\n",
       "      <td>0.017014</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.092677</td>\n",
       "      <td>0.153194</td>\n",
       "      <td>0.125276</td>\n",
       "      <td>0.265234</td>\n",
       "      <td>0.044371</td>\n",
       "      <td>0.056095</td>\n",
       "      <td>0.065345</td>\n",
       "      <td>0.041114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12817.000000</td>\n",
       "      <td>0.050336</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009310</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.677812</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.313636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27082.000000</td>\n",
       "      <td>0.192114</td>\n",
       "      <td>0.170455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010319</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.801045</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.064220</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>39460.000000</td>\n",
       "      <td>0.437919</td>\n",
       "      <td>0.390152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011585</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.094595</td>\n",
       "      <td>0.031579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>53042.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unique_ID   text_length    word_count      num_urls    num_emojis  \\\n",
       "count  45261.000000  45261.000000  45261.000000  45261.000000  45261.000000   \n",
       "mean   26237.805440      0.276083      0.247557      0.003230      0.001622   \n",
       "std    15279.138295      0.264196      0.236354      0.027201      0.018873   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    12817.000000      0.050336      0.045455      0.000000      0.000000   \n",
       "50%    27082.000000      0.192114      0.170455      0.000000      0.000000   \n",
       "75%    39460.000000      0.437919      0.390152      0.000000      0.000000   \n",
       "max    53042.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       num_special_chars  num_excess_punct  avg_word_length  stopword_ratio  \\\n",
       "count       45261.000000      45261.000000     45261.000000    45261.000000   \n",
       "mean            0.002778          0.003410         0.010863        0.198562   \n",
       "std             0.013427          0.017014         0.006173        0.092677   \n",
       "min             0.000000          0.000000         0.000000        0.000000   \n",
       "25%             0.000000          0.000000         0.009310        0.158273   \n",
       "50%             0.000000          0.000000         0.010319        0.206897   \n",
       "75%             0.000000          0.000000         0.011585        0.250000   \n",
       "max             1.000000          1.000000         1.000000        1.000000   \n",
       "\n",
       "       type_token_ratio      polarity  subjectivity    noun_ratio  \\\n",
       "count      45261.000000  45261.000000  45261.000000  45261.000000   \n",
       "mean           0.807242      0.504900      0.452480      0.012495   \n",
       "std            0.153194      0.125276      0.265234      0.044371   \n",
       "min            0.000000      0.000000      0.000000      0.000000   \n",
       "25%            0.677812      0.450000      0.313636      0.000000   \n",
       "50%            0.801045      0.500000      0.500000      0.000000   \n",
       "75%            1.000000      0.558333      0.614286      0.000000   \n",
       "max            1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "         verb_ratio     adj_ratio     adv_ratio  \n",
       "count  45261.000000  45261.000000  45261.000000  \n",
       "mean       0.056079      0.069837      0.021898  \n",
       "std        0.056095      0.065345      0.041114  \n",
       "min        0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.027778      0.000000  \n",
       "50%        0.051282      0.064220      0.000000  \n",
       "75%        0.078947      0.094595      0.031579  \n",
       "max        1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
